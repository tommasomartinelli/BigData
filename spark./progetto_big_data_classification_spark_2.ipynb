{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923e1538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, \\\n",
    "    auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13e9f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 17:05:18 WARN Utils: Your hostname, bigdata2022-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/11/26 17:05:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata2022/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/26 17:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Australian_rain_prediction').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc6c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv',inferSchema=True,header=True)\n",
    "#df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e40054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer = StringIndexer(inputCol=None, ouputCols=None).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc24f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'RainToday', 'RainTomorrow', 'Month', 'WindGustDir_N', 'WindGustDir_S', 'WindGustDir_W', 'WindDir3pm_N', 'WindDir3pm_S', 'WindDir3pm_W']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792c07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.drop('RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645704ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=df_1.columns, outputCol=\"features_to_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9452e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features_to_scale\", outputCol=\"features_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a330360e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final_assembler = VectorAssembler(inputCols=\"features_scaled\",outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e208f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GBTClassifier(labelCol=\"RainTomorrow\", featuresCol=\"features_scaled\", maxIter=10) \n",
    "#model = GradientBoostedTrees.trainClassifier(X_train_labled, categoricalFeaturesInfo={}, numIterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42524b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializing pipeline\n",
    "# stages = indexer\n",
    "# stages.append(assembler)\n",
    "# stages.append(scaler)\n",
    "# stages.append(model)\n",
    "# pipeline = Pipeline().setStages(stages)\n",
    "pipeline = Pipeline(stages=[assembler, scaler, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b8511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cc3087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = df.drop('RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d543f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = df.drop('RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063070bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ad793c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 17:05:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/11/26 17:05:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "22/11/26 17:05:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "22/11/26 17:05:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "22/11/26 17:05:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "22/11/26 17:05:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8b4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test_data)\n",
    "#predictionAndLabels = test_data.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b285562f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 17:05:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|prediction|RainTomorrow|\n",
      "+----------+------------+\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       1.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       1.0|           1|\n",
      "|       1.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       1.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       1.0|           1|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           1|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "|       0.0|           0|\n",
      "+----------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 17:05:55 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/11/26 17:05:55 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"RainTomorrow\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64c88f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 17:05:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "22/11/26 17:05:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      " Schema: _c0, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, RainToday, RainTomorrow, Month, WindGustDir_N, WindGustDir_S, WindGustDir_W, WindDir3pm_N, WindDir3pm_S, WindDir3pm_W\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata2022/datasets/big_data_project/australian_rain_dataset.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t_test=predictions.select('RainTomorrow').toPandas()\n",
    "t_hat=predictions.select('prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a27eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score on the test set:  0.8619049490939109\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy score on the test set: \", accuracy_score(t_test, t_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
